---
layout: post
title: "What the hell is word2vec model"
quote: "try to explain what I've learnt."
image: /media/2014-10-27-bingot/cover.jpg
video: false
comments: true
---

Whatever you want a machine to learn, the first step is always feeding it with data. In what format should the data be prepared, that is a question. When it comes to image processing problems, it is just natural to feed the machine learning model with raw pixels. A pixel is often three integer values R, G and B in the range of [0, 255]. Or it can be three floats between [0.0, 1.0]. Those numbers are not random, they have meanings. They convey the intensity of each color. For example, 255 means the highest intensity, or white. 0 means the lowest intensity, or black. But what about language processing problems, what should be used for words, the basic unit of text.

The most straightforward way is of course using ASCII code. After all we have been using it everywhere for string representation. But ASCII is not ideal in that it doesn't convey any meanings of the words it represents. For example, the ASCII for "cat" is 0x636174 and the ASCII for "dog" is 0x646F67. If you only look at the ASCII codes, you will have no clue that "cat" and "dog" are two close concepts, as both are domestic animals. How can we have a better word representation which captures the meaning of words, so that the machine won't learn from meaningless numbers, but has a sense of meaning from the beginning? That's the problem word2vec is trying to solve.

But what's meaning? Well, I think it is really difficult to have an absolute answer and I think the meaning of a word if very subjective too. But it is easy to see the relative differences between two meanings. For example, we know that the word "Good" and "Bad" are very different. But the word "Great" is somewhat similar to "Good". How would you explain the meaning of "Great"? You would probably say it means "Very Good". So how about defining meanings as categories? Words have the meaning of "Good" fall into one category whereas words close to "Bad" are into another category. But soon you will see that there is no clear boundary between categories. You have "excellent", "splendid", "nice", "fine", and you have "neutral", "ok" and "so so" sit in between, and "wrong", "awkward", "poor" that are clearly belong to the other category. So really they are just locations. Close meanings are located closer whereas different meanings stay apart. That's the idea of word2vec - locating similar meanings together.

But to locate words based on meanings, we need to somehow understand meanings. It is easy for a human, but difficult for a machine. But we know that words of a similar meaning can be interchanged in a context. For example, the word "cat" in "I pat my cat." can be changed to "dog". The context "I pat my" defines certain features of the last word that both cat and dog can fit. For example, the word "pat" indicates that the thing is "patible", something cute perhaps, or at least not wild or dangerous. The word "my" suggests that the thing is “ownable”, so perhaps domestic. As another example, "I feed my cat" or "I feed my dog" tell you that both cat and dog can eat, so they must be living things. So if we define a word based on its neighboring words (context), in the end we should find that similar meanings apear in similar contexts are they are interchangeable. Using a word's contexts to define its meaning, that's the basic idea.
